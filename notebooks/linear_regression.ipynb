{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code just sets up this notebook environment. Just run it once and then ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell once to setup the notebook environment, then ignore it\n",
    "\n",
    "# set the path\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# load the notebook's style sheet\n",
    "from IPython.core.display import HTML\n",
    "css_file = 'style.css'\n",
    "HTML(open(css_file, \"r\").read())\n",
    "\n",
    "# embed plots\n",
    "%matplotlib inline\n",
    "\n",
    "from regression.toolkit import generate_linear_data, plot, print_residuals, np\n",
    "from regression.toolkit import huber, SGDRegressor, generate_quadratic_data, generate_unscaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "Let's take a look at the following this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = generate_linear_data(slope=1, intercept=10, noise_std=10)\n",
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had to represent all these data points in one single line, how would the line would look like?\n",
    "Well, that's what a linear regression does: It finds the line whose distance to all the datapoints is minimum. These distace are called residuals.\n",
    "\n",
    "Concretelly, in the above dataset a linear regression is a linear function of the type: \n",
    "\n",
    "$$f(x) = \\theta_0 + \\theta_1 \\cdot x$$\n",
    "\n",
    "where $\\theta_0$ and $\\theta_1$ are the coefficients that set the position and slope of the linear function. In the general case a regresion has n coefficients:\n",
    "\n",
    "$$f(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ... + \\theta_n \\cdot x_n = \\theta_0 + \\sum_{i=1}^n\\theta_i \\cdot x_i$$\n",
    "\n",
    "However, in order to make this explanation easier, for now we will focus on a regression with two coeficients only: $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "Now the question is, how do we find the $\\theta_0$ and $\\theta_1$ that minimize the residuals? One option is to try different values for $\\theta_0$ and $\\theta_1$ until we find the one that minimizes these residuals.\n",
    "\n",
    "Well, let's to do that.\n",
    "\n",
    "In the below box, find the values of $\\theta_0$ and $\\theta_1$ that minimize the residuals. Keep in mind that when the position of the line is too high, then $\\theta_0$ should be lower, and when the slope is too high, then $\\theta_1$ should be lower. This also applies in the oposite scenario.\n",
    "\n",
    "As an example, we start with $\\theta_0 = 60$ and $\\theta_1 = 0.08$.\n",
    "\n",
    "(Keep in mind that in our Python implementation below x is a vector that has the example datapoints and predictions is the vector with the predictions. The regression gives a prediction for each element in x and stores it in predictions using the same order as in x.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0 = 20\n",
    "theta1 = 0.08\n",
    "\n",
    "predictions = theta0 + theta1*x\n",
    "plot(x, y, predictions=predictions)\n",
    "print_residuals(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that there is no need to randomly try different values. Let's say that we start with random values $\\theta_0$ and $\\theta_1$ and that we somehow we can learn if these values need to increase or decrease, and what should be the magnitude of that increase or decrease. Then, after we update these values, we do that again. This goes on until $\\theta_0$ and $\\theta_1$ are the coefficients that minimize the distance between the datapoints and our line (i.e. the optimal coefficients).\n",
    "\n",
    "In general this procedure is called an algorithm. In particular this algorithm is called gradient descent.\n",
    "\n",
    "Before we dive into gradient descent we need to define what are we trying to minimize here.\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "In our example, we were trying to minimize the residuals, i.e. distances between all datapoints and our line, and that is a function of the parameters $\\theta_0$ and $\\theta_1$. We call that function $J(\\theta_0, \\theta_1)$.\n",
    "\n",
    "Does that makes sense? think about it... when we updated $\\theta_0$ and $\\theta_1$ the distance between the datapoints and the line was modified. It bacame smaller at the beggining until we missed the optimum values for $\\theta_0$ and $\\theta_1$ and then it became larger again. That is us poking with $J(\\theta_0, \\theta_1)$ by testing different values of $\\theta_0$ and $\\theta_1$. We were trying to find the optimal (minimum) value.\n",
    "\n",
    "In our example, $J(\\theta_0, \\theta_1)$ was defined as the summation of the residuals, which is equal to:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = |y^{(1)} - f(x^{(1)})| + |y^{(2)} - f(x^{(2)})| +...+ |y^{(n)} - f(x^{(n)})|$$\n",
    "\n",
    "where $y^{(i)}$ is our target for $x^{(i)}$ $\\forall i$ and $n$ is the number of datapoints in the training set. If we expand $f(x)$ we get:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = |y^{(1)} - (\\theta_0 + \\theta_1 \\cdot x_1^{(1)})| + |y^{(2)} - (\\theta_0 + \\theta_1 \\cdot x_1^{(2)})| +...+ |y^{(n)} - (\\theta_0 + \\theta_1 \\cdot x_1^{(n)})|$$\n",
    "\n",
    "and since we are doing a lot of adding, let's introduce summations:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\sum_{i=1}^n|y^{(i)} - (\\theta_0 + \\theta_1 \\cdot x_1^{(i)})|$$\n",
    "\n",
    "\n",
    "### General case\n",
    "\n",
    "We have only seen the case where each of our examples have a single feature, i.e. each example is represented by a number. For example, we may want to predict the price of a house given its size. Since the size of the house can be represented by a single number (its area), this is a one feature regression.\n",
    "\n",
    "However, we can use more features. What if we wanted to predict the house price given its size and its distance to the downtown? That would make our x become a matrix X (note the capital) with two columns where each example is a row and one column is the size of the house and the other column is the distance to downtown.\n",
    "\n",
    "In the general case with n features our loss function becomes:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^n|y^{(i)} - x^{(i)} \\cdot \\theta|$$\n",
    "\n",
    "Where $\\theta$ is a vector with coefficients $\\theta_0, \\theta_1, ...,\\theta_n$, and $x^{(i)}$ is a row in the matrix X. \n",
    "\n",
    "**Challenge**: For the above equation to make sense we need to append a column of ones to the left of matrix X. Why? Write your answer in the box below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the loss function of a linear regression that minimizes the residuals. This function is called **Least Absolute Loss** and when minimized gives the parameters $\\theta_0$ and $\\theta_1$ that create a **Least Absolute Regression**.\n",
    "\n",
    "However, classic linear regression assumes that the residuals are normaly distributed. This means the loss function is not the Least Absolute Loss, but the **Least Squared Loss**, which is the sum of the squared residuals:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\sum_{i=1}^n(y_i - (\\theta_0 + \\theta_1 \\cdot x_1))^2$$\n",
    "\n",
    "Minimizing the **Least Squared Error** gives us a **Least Squared Regression**. Let's see how the Least Squared Error function looks in our dataset when setting $\\theta_0=0$ and using $\\theta_1$ as our only independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0 = 0\n",
    "theta1 = np.arange(-2, 4.3, 0.01)\n",
    "squared_residuals = []\n",
    "for t in theta1:\n",
    "    predictions = theta0 + t * x\n",
    "    squared_residuals.append(np.sum(np.square(y - predictions)))\n",
    "plot(theta1, squared_residuals, y_label='y^2', line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**: How would you prove that the Lease Squared Loss is the cost function when residuals are normally distributed? Write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is mostly about modeling a dataset by creating a convex error function and minimizing it.\n",
    "\n",
    "Since the Least Squares Loss uses the squares of the residuals, it penalizes heavily on large residuals, and not so much on small residuals.\n",
    "\n",
    "Another popular loss function used in regression is the **Huber loss**, which doesn't penalize outliers as heavily as the Least Squares loss.\n",
    "\n",
    "\\begin{align}\n",
    "J_{huber}(\\theta_0, \\theta_1) = \\left\\{ \\begin{array}{cl}\n",
    "\\frac{1}{2} \\left[y-f(x)\\right]^2 & \\text{for }|y-f(x)| \\le \\delta, \\\\\n",
    "\\delta \\left(|y-f(x)|-\\delta/2\\right) & \\text{otherwise.}\n",
    "\\end{array}\\right.\n",
    "\\end{align}\n",
    "\n",
    "The Huber loss is basicaly a mix between the two loss functions we have seen so far: a Least Squares Loss when the residuals are small and an Absolute Loss when the residuals are large. $\\delta$ is the threshold that defines when a residual is small or large. Let's see how the Huber Loss looks like in our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = 50\n",
    "theta0 = 0\n",
    "theta1 = np.arange(-2, 4.3, 0.01)\n",
    "huber_residuals = []\n",
    "for t in theta1:\n",
    "    predictions = theta0 + t * x\n",
    "    huber_residuals.append(np.sum(huber(y - predictions, delta)))\n",
    "plot(theta1, huber_residuals, y_label='huber', line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**: The Huber Loss is very usefull when there are outliers in the dataset. Why? Write your answer in the box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Now that we understand loss functions, we can look into how to minimize them. Gradient descent is an algorithm that iterates (i.e. \"walks\") thru a functions until it reaches a minimum value. The algorithm starts with a solution located at a random point on the loss function $J(\\theta)$ and does the following:\n",
    "\n",
    "1) calculate the slope of $J(\\theta)$\n",
    "\n",
    "2) update the current solution by moving it over $J(\\theta)$ in the oposite direction of the slope.\n",
    "\n",
    "The algorithm does 1) and 2) until $J(\\theta)$ stops changing (in practice this means $J(\\theta)$ changes very little).\n",
    "\n",
    "We won't implement gradient descent in this class, but it's good that you have an intuition of how this optimization algorithm works.\n",
    "\n",
    "**Challenge**: would gradient descent find the minimum value in the function below? Write your answer in the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_ = np.arange(-12, 12, 0.1)\n",
    "plot(x_, np.multiply(np.cos(x_), np.square(x_)), line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a linear regression machine learning algorithm using scikit learn\n",
    "\n",
    "scikit learn is a cool Python package. It lets us implement complex algortithms without dealing with the low level implementation details like gradient descent. However we need to know how it works under the hoods so we can tune our algorithms wisely if they don't perform well.\n",
    "\n",
    "The main scikit learn class we will use for regression is called SGDRegressor. You can check the full API [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html), but the important bits are:\n",
    "\n",
    "SDGRegresor() returns an object that is an untrained regression. This means that the regression hasn't been optimized yet so it doesn't have valid coeficients. Let's save such a regression in variable model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor(loss='squared_loss',  n_iter=10, eta0=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the model are:\n",
    "\n",
    "* loss is the loss function to use. In this class we will only use squared_loss and huber.\n",
    "* n_iter is the number of times the algorithm with iterate thru the dataset. Usually the more the better, but there is an acuracy vs. running time tradeoff.\n",
    "* eta0 is the learning rate or step size. The smaller the step size the more likely the algorithm will converge, but its convergence will take longer.\n",
    "\n",
    "In order to find the optimum parameters we need to train our regression model. We do that the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our variable model has a trained model object that has valid coefficients. In order to use these coeficients to make a prediction we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable predictions now has our, well, predictions. Let's plot our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x, y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happened? We solved the initial problem of findinf the right line without having to test several coefficients by hand. SGDRegressor took care of it. The only things we had to test were the loss function (loss), the number of iterations (n_iter) and the learning rate (eta0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folowing dataset has outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = generate_linear_data(slope=1, intercept=0, noise_std=1, max_x=10)\n",
    "y[0] = 10\n",
    "y[y.shape[0]-1] = 0\n",
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SGDRegressor to make a least squares regression on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor(loss='squared_loss', eta0=0.01, n_iter=10000)\n",
    "model.fit(x, y)\n",
    "predictions = model.predict(x)\n",
    "plot(x, y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**: Why does the regression look so unacurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the below box, you create your own regresion to make reasonable predictions with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your code here and assign a value to predictions.\n",
    "predictions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x, y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset has quadratic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = generate_quadratic_data(a=1, b=10, c=1, noise_std=500)\n",
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make predictions using SGDRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your code here and assign a value to predictions.\n",
    "predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(x, y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
